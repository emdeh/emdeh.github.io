<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Using Retrieval Augmented Generation (RAG) for chatbots | emdeh</title> <meta name="author" content=" "> <meta name="description" content="A simple example of how RAG can be used for a website's chatbot."> <meta name="keywords" content="cybersecurity, ai, technology cyber"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%A8%E2%80%8D%F0%9F%92%BB&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://emdeh.github.io/blog/2024/rag-llm-chatbot/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-JZNPNDMG9P"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-JZNPNDMG9P");</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">emdeh</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/library/">Library</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/blog/category/htb-machines">HTB-Machines</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/blog/category/essential-eight/">Essential Eight</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/blog/category/artificial-intelligence/">Artificial Intelligence</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/blog/category/explainers/">Explainers</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Using Retrieval Augmented Generation (RAG) for chatbots</h1> <p class="post-meta">February 16, 2024</p> <p class="post-tags"> <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/rag"> <i class="fas fa-hashtag fa-sm"></i> RAG</a>   <a href="/blog/tag/llm"> <i class="fas fa-hashtag fa-sm"></i> LLM</a>   <a href="/blog/tag/nlp"> <i class="fas fa-hashtag fa-sm"></i> NLP</a>   <a href="/blog/tag/natural-language-processing"> <i class="fas fa-hashtag fa-sm"></i> natural-language-processing</a>   <a href="/blog/tag/retrieval-augmented-generation"> <i class="fas fa-hashtag fa-sm"></i> retrieval-augmented-generation</a>   <a href="/blog/tag/large-language-models"> <i class="fas fa-hashtag fa-sm"></i> large-language-models</a>   <a href="/blog/tag/chatbot"> <i class="fas fa-hashtag fa-sm"></i> chatbot</a>   <a href="/blog/tag/python"> <i class="fas fa-hashtag fa-sm"></i> python</a>   <a href="/blog/tag/embeddings"> <i class="fas fa-hashtag fa-sm"></i> embeddings</a>     ·   <a href="/blog/category/artificial-intelligence"> <i class="fas fa-tag fa-sm"></i> Artificial-Intelligence</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="introduction">Introduction</h1> <p>This project leverages a Retrieval Augmented Generation (RAG) implementation to create an intelligent question-answering system for a website. The project automates the collection of contextual data from the site, processes this data with an embeddings model to generate vector representations, and utilises these vectors to provide relevant answers to user queries through a chatbot using a Language Model (LLM) to craft responses in a conservational tone.</p> <p>You can find the code and a detailed overview in the <a href="https://github.com/emdeh/web-crawl-qna-blog-bot" rel="external nofollow noopener" target="_blank">Github repository</a>.</p> <h2 id="what-is-retrieval-augmented-generation-rag">What is Retrieval Augmented Generation (RAG)</h2> <p>Retrieval Augmented Generation (RAG) is a sophisticated approach that enhances the capabilities of generative models, particularly Large Language Models (LLMs), by integrating an additional information retrieval step into the response generation process. This method involves dynamically sourcing relevant external information to augment the input provided to the generative model, thereby enriching its responses with details and insights not contained within its pre-trained knowledge base. The retrieval of additional information is typically facilitated by embeddings and vector representations to identify content contextually similar to the user’s prompt.</p> <h2 id="what-are-embeddings">What are Embeddings</h2> <p>Embeddings are a form of representation learning where words, sentences, or even entire documents are converted into real-valued vectors in a high-dimensional space. This process aims to capture the semantic meanings, relationships, and context of words or phrases, allowing machines to process natural language data more effectively. The vectors in the high-dimensional space represent the nuanced characteristics of the text, such as syntax, semantics, and usage patterns, in a form that can be quantitatively analysed. Each dimension could correspond to a latent feature that captures different aspects of the text’s meaning, not directly interpretable by humans but discernible through computational methods. By mapping textual information to a geometric space, embeddings enable the measurement of conceptual similarity between pieces of text based on their positions and distances within this space, facilitating tasks like search, classification, and contextual understanding in natural language processing applications. In the context of Retrieval-Augmented Generation (RAG), embeddings represent the queries (prompts) and the potential knowledge sources in a format that a computer can understand and compare.</p> <h3 id="vector-representations">Vector Representations</h3> <p>Vector representations are the outcome of converting text into embeddings, representing text as points or vectors in a multi-dimensional space. As described above, each dimension corresponds to a feature of the text, capturing various aspects of its meaning, context, or syntactical properties. Comparing vector representations involves calculating the similarity (often using cosine similarity or other metrics) between vectors to identify how closely related two pieces of text are. In RAG implementations that use embeddings, the vector representation of a user’s prompt is compared to the vector representations of various knowledge sources to identify the most relevant context. This relevant context is then retrieved and used to augment the response generated by a language model, enhancing the LLM’s ability to provide accurate and contextually enriched answers.</p> <h2 id="credits">Credits</h2> <p>This project was initially inspired by <strong>OpenAI’s Web Q&amp;A with Embeddings tutorial</strong>. Learn how to crawl your website and build a Q/A bot with the OpenAI API. You can find the full tutorial in the <a href="https://platform.openai.com/docs/tutorials/web-qa-embeddings" rel="external nofollow noopener" target="_blank">OpenAI documentation</a>.</p> <h1 id="overview-of-a-rag-implementation">Overview of a RAG implementation</h1> <p>The diagram below briefly outlines how a Retrieval Augmented Generation (RAG) architecture leverages embeddings. In short, additional context is <em>retrieved</em> by comparing the vectors of the prompt to the vectors of the knowledge source. The related textual data is then appended to the prompt to <em>augment</em> the response <em>generated</em> by the LLM.</p> <p><img src="/assets/img/2024-rag-chatbot/diagram.png" alt="diagram"></p> <h1 id="example-implementation">Example implementation</h1> <p><strong>Point 1:</strong> In the case of this particular implementation, the knowledge source is a blog. The knowledge is obtained by first extracting all the hyperlinks on the site and discarding any that point to other domains. Each unique hyperlink is then visited, and the content extracted into text files. The text files are then used to create a data frame. Each row in the data frame is tokenised to facilitate analysing the length of documents, which is relevant for understanding the data’s distribution and optimising model input sizes.</p> <p><strong>Point 2:</strong> After more processing to create smaller chunks (if required), the embeddings are generated and saved. In this case, to a <code class="language-plaintext highlighter-rouge">.csv</code> file.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;SNIP&gt;
https://emdeh.com/repositories
https://emdeh.com/news/announcement_7
https://emdeh.com/blog/2024/codify-walkthrough
Embeddings generated and saved to <span class="s1">'data/embeddings.csv'</span><span class="nb">.</span>
Preprocessing complete. Embeddings are ready.

<span class="c"># You can see the blog's links being iterated here.</span>
</code></pre></div></div> <p><strong>Points 3 - 5:</strong> When a user provides the prompt to the service, the embeddings model will generate its vector representation.</p> <p><img src="/assets/img/2024-rag-chatbot/image-of-prompt.png" alt="image of prompt"></p> <p><strong>Point 6:</strong> The service then compares the prompt’s vector to the Vector DB (in this case, the <code class="language-plaintext highlighter-rouge">.csv</code> file containing the blog’s vector representations is loaded into another data frame).</p> <blockquote> <p><em>The comparision is done using Cosine function to calculate the distance between the question’s embedding and each row’s embedding in the data frame. Cosine distances is a measure used to determine the similarity between two vectors, with lower values indicating higher similarity.</em></p> </blockquote> <p>The service will then iterate over the data frame to accumulate the most similar text until it reaches a pre-defined token limit. This then forms the context for the original prompt.</p> <p><strong>Points 7 - 9:</strong> The context and original prompt are now passed to the GPT model, which returns a generative completion. This completion is presented back to the end-user.</p> <p><img src="/assets/img/2024-rag-chatbot/image-of-completion.png" alt="image of completion"></p> <h1 id="code-overview">Code overview</h1> <h2 id="data-collection-and-preparation">Data Collection and Preparation</h2> <p><code class="language-plaintext highlighter-rouge">preprocess.py</code> crawls web pages within a specified domain and systematically navigates through the website, extracting text from each page it encounters. The collected text undergoes initial preprocessing to clean and organise the data, making it suitable for further analysis.</p> <p>The script then employs OpenAI’s API to generate embeddings for each piece of text. These embeddings capture the semantic essence of the text in a high-dimensional space, facilitating the identification of contextual similarities between different texts. The processed data and its embeddings are saved for subsequent use, laying the groundwork for the system’s question-answering capabilities.</p> <h2 id="flask-application-for-question-answering">Flask Application for Question Answering</h2> <p>With the data prepared, <code class="language-plaintext highlighter-rouge">app.py</code> serves as the interface between the user and the system’s NLP engine. This script initiates a Flask web application, providing endpoints for users to submit their questions.</p> <p>Upon receiving a query, the application leverages the previously generated embeddings to find the most relevant context within the collected data. It then formulates this context and the user’s question as input for an OpenAI GPT model. The model, trained on vast amounts of text from the internet, generates an answer that reflects the specific information in the crawled data and its understanding of the topic at large. The answer is then returned to the user through the web interface, completing the cycle of query and response.</p> <h2 id="integration-and-workflow">Integration and Workflow</h2> <p>Integrating <code class="language-plaintext highlighter-rouge">preprocess.py</code> and <code class="language-plaintext highlighter-rouge">app.py</code> creates a workflow that bridges web crawling and NLP-driven question-answering. Initially, <code class="language-plaintext highlighter-rouge">preprocess.py</code> lays the foundation by collecting and preparing the data, which <code class="language-plaintext highlighter-rouge">app.py</code> subsequently utilises to offer real-time answers. This allows the system to provide contextually relevant answers informed by the specific context. Users interact with the system through a straightforward web interface, making complex NLP capabilities accessible to anyone with a question to ask.</p> <h2 id="use-cases">Use-cases</h2> <p>Together, these scripts leverage sophisticated machine learning capabilities to demonstrate how existing data from websites can be harnessed to build robust and interactive AI-driven ways to retrieve and discover knowledge.</p> <p>For example, the basic capabilities demonstrated in this project could be applied to create a contextually-aware chatbot on a website.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/codify-walkthrough/">Codify</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/bizness-walkthrough/">Bizness</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/devvortex-walkthrough/">Devvortex</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/essential-eight-application-control/">Application Control</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/assessment-planning-scoping/">Assessment planning and scoping</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-JZNPNDMG9P"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-JZNPNDMG9P");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>