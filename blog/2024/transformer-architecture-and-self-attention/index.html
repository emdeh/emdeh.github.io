<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>An overview of Transformer architecture and self-attention | emdeh</title> <meta name="author" content=" "> <meta name="description" content="A high-level explanation on Transformers and the role of self-attention."> <meta name="keywords" content="cybersecurity, ai, technology cyber"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%A8%E2%80%8D%F0%9F%92%BB&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://emdeh.github.io/blog/2024/transformer-architecture-and-self-attention/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-JZNPNDMG9P"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-JZNPNDMG9P");</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">emdeh</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/library/">Library</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/blog/category/htb-machines">HTB-Machines</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/blog/category/essential-eight/">Essential Eight</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/blog/category/artificial-intelligence/">Artificial Intelligence</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/blog/category/explainers/">Explainers</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">An overview of Transformer architecture and self-attention</h1> <p class="post-meta">March 18, 2024</p> <p class="post-tags"> <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/transformers"> <i class="fas fa-hashtag fa-sm"></i> transformers</a>   <a href="/blog/tag/transformer-architecture"> <i class="fas fa-hashtag fa-sm"></i> transformer-architecture</a>   <a href="/blog/tag/self-attention"> <i class="fas fa-hashtag fa-sm"></i> self-attention</a>   <a href="/blog/tag/encoders"> <i class="fas fa-hashtag fa-sm"></i> encoders</a>   <a href="/blog/tag/decoders"> <i class="fas fa-hashtag fa-sm"></i> decoders</a>   <a href="/blog/tag/sequence-to-sequence"> <i class="fas fa-hashtag fa-sm"></i> sequence-to-sequence</a>   <a href="/blog/tag/encoder-decoder"> <i class="fas fa-hashtag fa-sm"></i> encoder-decoder</a>   <a href="/blog/tag/auto-regression"> <i class="fas fa-hashtag fa-sm"></i> auto-regression</a>     ·   <a href="/blog/category/artificial-intelligence"> <i class="fas fa-tag fa-sm"></i> Artificial-Intelligence</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>In Natural Language Processing (NLP), a transformer architecture is a type of deep learning model that has significantly improved the ability to understand and generate human language. Vaswani et al. introduced transformers in the paper “Attention is All You Need” in 2017 and distinguished them by their application of self-attention mechanisms. Self-attention mechanisms enable a model to weigh the importance of different words within a sentence, regardless of their positional distance from each other.</p> <p><strong><em>Key Features of Transformers</em></strong></p> <ul> <li> <strong>Self-Attention:</strong> allows the model to dynamically focus on different parts of an input as it processes information, enabling it to effectively capture context and relationships between words.</li> <li> <strong>Parallel Processing:</strong> Transformers can process entire sequences of data in parallel, which significantly speeds up training and improves the model’s ability to handle long sequences. Previous sequence models like RNNs (Recurrent Neural Networks) and LSTMs (Long-Short-Term Memory Networks) could only process data sequentially.</li> <li> <strong>Layered Structure:</strong> Transformers comprise multiple layers of self-attention and feed-forward neural networks. A layered structure enables Transformers to learn complex patterns and relationships in the data, which is critical to the depth of their performance on a broad range of NLP tasks.</li> <li> <strong>Scalability:</strong> Due to parallel processing and efficient training on large datasets, transformers are highly scalable, making them suitable for cases requiring an understanding of complex and nuanced language.</li> </ul> <p><strong><em>Applications</em></strong></p> <p>Many state-of-the-art NLP models, such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pretrained Transformer), have a Transformer foundation. These models have set new benchmarks in various NLP tasks, such as text classification, machine translation, question answering, and text generation.</p> <p>The transformer model’s ability to understand context and nuance in text has enabled the development of more sophisticated and interactive AI applications, and it is a cornerstone of modern NLP research.</p> <h1 id="the-architecture">The architecture</h1> <p><br> Transformer architectures have three broad models:</p> <ul> <li>Encoders</li> <li>Decoders, and</li> <li>Encoder-Decoders (Sequence-to-Sequence)</li> </ul> <h2 id="encoders">Encoders</h2> <p>Encoders in transformers process input text into a format (vector representations) that captures the essence of the original information.</p> <blockquote> <p><strong><em>Encoder models are bidirectional.</em></strong></p> </blockquote> <p>Because encoders consider the context from both before and after a given word within the same layer, they are said to be <strong>bi-directional</strong>. Bi-directional capability contrasts with traditional models that process input in a strict uni-directional sequence (either left-to-right or right-to-left). Thus, it could only incorporate context from one direction at a time in their initial layers.</p> <p>Imagine the sentence, <em><code class="language-plaintext highlighter-rouge">The cat sat on the mat.</code></em> Bidirectionality means that when processing the word <em><code class="language-plaintext highlighter-rouge">sat</code></em>, the encoder considers the context of <em><code class="language-plaintext highlighter-rouge">The cat</code></em> (words before <em><code class="language-plaintext highlighter-rouge">sat</code></em>) and <em><code class="language-plaintext highlighter-rouge">on the mat</code></em> (words after <em><code class="language-plaintext highlighter-rouge">sat</code></em>) simultaneously. This allows the encoder to understand that <em><code class="language-plaintext highlighter-rouge">sat</code></em> is an action performed by <em><code class="language-plaintext highlighter-rouge">the cat</code></em> and it occurred <em><code class="language-plaintext highlighter-rouge">on the mat</code></em>, integrating full-sentence context into its representation of <em><code class="language-plaintext highlighter-rouge">sat</code></em>.</p> <p>In contrast, <strong>unidirectional</strong> models, such as decoders (see below), would only consider “<em><code class="language-plaintext highlighter-rouge">The cat</code></em> when first encountering <em><code class="language-plaintext highlighter-rouge">sat</code></em>, meaning it misses the contextual clues provided by <em><code class="language-plaintext highlighter-rouge">on the mat</code></em> until later layers, or not at all, depending on the model’s overall architecture.</p> <p>Bi-directional processing enables transformers to capture a more nuanced and complete understanding of language, which makes them particularly effective for tasks that require a deep understanding of context, such as sentence classification, sentiment analysis, and named entity recognition.</p> <blockquote> <p><strong><em>Encoders use self-attention layers to understand relative context.</em></strong></p> </blockquote> <p>Encoders in transformer models aim to evaluate and understand each part of the input text relative to the entire text. This is achieved by first converting each word or part of the input into a vector representation using embeddings. For each of these vector representations, the model generates three distinct vectors: <em>Query <code class="language-plaintext highlighter-rouge">(Q)</code></em>, <em>Key <code class="language-plaintext highlighter-rouge">(K)</code></em>, and <em>Value <code class="language-plaintext highlighter-rouge">(V)</code></em>. The <code class="language-plaintext highlighter-rouge">Q</code>, <code class="language-plaintext highlighter-rouge">K</code>, and <code class="language-plaintext highlighter-rouge">V</code> vectors are then utilised to calculate attention scores, determining the weight each word’s representation should assign to every other word’s representation in the input. This weighting process enables the model to determine how much ‘attention’ or importance each part of the input should give to other parts, effectively allowing each word to consider the context provided by the entire input. This mechanism, known as <strong>self-attention</strong>, is pivotal for the model’s ability to capture and utilise contextual information within the input.</p> <p>Encoder-only models are often used in tasks that require an understanding of the input, like sentence classification or named entity recognition.</p> <h2 id="decoders">Decoders</h2> <blockquote> <p><strong><em>Decoders use a masked self-attention layer.</em></strong></p> </blockquote> <p>Self-attention in decoders is said to be <strong>masked</strong>. Masking prevents a decoder from ‘seeing’ future parts of the sequence during training, ensuring each word prediction is based only on already generated words. In other words, during the generation of an output sequence, each position can only attend to positions that preceded the current position in the sequence. This constraint is crucial for text generation, where models predict the next word based on the previous ones.</p> <p>For example, imagine the decoder is generating the text <em><code class="language-plaintext highlighter-rouge">The quick brown fox.</code></em> When it’s predicting the word after <em><code class="language-plaintext highlighter-rouge">The quick,</code></em> the masked self-attention mechanism allows the decoder to consider <em><code class="language-plaintext highlighter-rouge">The</code></em> and <em><code class="language-plaintext highlighter-rouge">quick</code></em> but not <em><code class="language-plaintext highlighter-rouge">brown</code></em> or <em><code class="language-plaintext highlighter-rouge">fox</code></em> because those words are in the future relative to the current position being predicted. This masking effectively enforces a uni-directional flow of information, ensuring that the model generates each word based solely on preceding words, preserving the natural order of text generation.</p> <blockquote> <p><strong><em>Because of masked self-attention, decoders are uni-directional.</em></strong></p> </blockquote> <p>They generate output one element at a time in a forward direction. In decoders, the future context is deliberately obscured to mimic the process of creating language one word at a time, making the decoding process fundamentally uni-directional.</p> <p>If decoders were not uni-directional and could instead attend to the entire input sequence indiscriminately (similar to encoders), the integrity of the generated output sequence would be compromised. Specifically, the following issues could arise:</p> <ul> <li> <em>Loss of Sequential Generation Logic:</em> Predicting the next word becomes moot if the decoder has access to future words, undermining the process of sequential text generation.</li> <li> <em>Incoherent or Circular Outputs:</em> Due to premature knowledge of future context, outputs might repeat or loop without a logical progression.</li> <li> <em>Compromised Learning Objective:</em> The model’s focus shifts from generating text based on learned structures to merely matching patterns, diluting the essence of language generation.</li> </ul> <blockquote> <p><strong><em>The generation of each element of the output sequence one at a time is Auto-Regression.</em></strong></p> </blockquote> <p>Generated each element of the output one at a time, based on the previously generated elements, is known as <strong>Auto-Regression</strong>. The auto-regressive property necessitates the use of masked self-attention in the decoder, as it relies on the premise that each step in the generation process only has access to previous steps.</p> <p>In summary, decoders are <em>uni-directional</em> because their <em>self-attention</em> layer is masked. Masking supports the <em>auto-regressive</em> nature of the generation process, ensuring that each step in generating the output can only use information from the steps that have already occurred.</p> <p>Decoder-only models are particularly useful at generative tasks, like text generation.</p> <h2 id="encoders-decoders">Encoders-decoders</h2> <p>Are also known as <strong>sequence-to-sequence</strong>. These models are good for generative tasks that are based on an input, such as translation or summarisation.</p> <h1 id="self-attention-layers">Self-Attention Layers</h1> <p><strong>Attention layers</strong> refers to any layer within a neural network that applies some form of the <em>attention mechanism</em>. Attention mechanisms allow models to focus on different parts of the input data with varying degrees of emphasis.</p> <blockquote> <p><strong><em>Self-Attention is one type of attention mechanism.</em></strong></p> </blockquote> <p>Self-Attention in transformer models enables each position in the input sequence to attend to all positions within the same sequence. Self-Attention enables transformers to process and interpret sequences of input data, such as sentences in natural language processing (NLP) and dynamically weigh the relevance of all parts of the input data against every other part when processing any single part, enabling the incorporation of relatively weighted context from the entire sequence.</p> <p>In other words, self-attention allows a model to understand the relationships between words, regardless of their positional distance. Here’s a more detailed look at how self-attention works:</p> <p>For example, imagine the sentence: <em><code class="language-plaintext highlighter-rouge">The cat purrs.</code></em></p> <p><strong>Step 1 - Input representation</strong><br> First, each word in the sentence (<em><code class="language-plaintext highlighter-rouge">The</code></em>, <em><code class="language-plaintext highlighter-rouge">cat</code></em>, <em><code class="language-plaintext highlighter-rouge">purrs</code></em>) is converted into a vector using embeddings. These vectors contain each word’s initial context.</p> <p><strong>Step 2 - Query, Key, and Value Vectors</strong><br> For each word, three vectors are generated from its embedding: a Query vector (<code class="language-plaintext highlighter-rouge">Q</code>), a Key vector (<code class="language-plaintext highlighter-rouge">K</code>), and a Value vector (<code class="language-plaintext highlighter-rouge">V</code>). This is done through linear transformations, which essentially means multiplying the word’s embedding by different weight matrices for <code class="language-plaintext highlighter-rouge">Q</code>, <code class="language-plaintext highlighter-rouge">K</code>, and <code class="language-plaintext highlighter-rouge">V</code>.</p> <p><strong>Step 3 - Calculating attention scores</strong><br> The “dot product” of the Query vector for <code class="language-plaintext highlighter-rouge">purrs</code> is calculated with the Key vector of every word in the sentence, including itself. Calculating the dot product with the Key vector (<code class="language-plaintext highlighter-rouge">K</code>) of every other word produces scores that represent how much attention <code class="language-plaintext highlighter-rouge">purrs</code> should pay to each word in the sentence, including <code class="language-plaintext highlighter-rouge">The</code> and <code class="language-plaintext highlighter-rouge">cat</code>.</p> <p><strong>Step 4 - Softmax to Determine Weights</strong><br> These scores converted into weights that sum to 1 through a mathematical normalisation process (a softmax function). The weights quantify the relevance of each word’s information to the word <code class="language-plaintext highlighter-rouge">purrs</code>.</p> <p><strong>Step 5 - Weighted Sum and Output</strong><br> The weights are used to create a weighted sum of the Value vectors, which incorporates information from the entire sentence into the representation of <code class="language-plaintext highlighter-rouge">purrs</code>. For instance, the high weight of <code class="language-plaintext highlighter-rouge">cat</code> (since it’s directly related to <code class="language-plaintext highlighter-rouge">purrs</code>) ensures that <code class="language-plaintext highlighter-rouge">purrs</code> is understood in the context of <em><code class="language-plaintext highlighter-rouge">The cat</code></em>, reinforcing that it’s the cat doing the purring.</p> <blockquote> <p><strong><em>The result is contextual representation.</em></strong></p> </blockquote> <p>Thanks to the self-attention mechanism, the output vector for “purrs” now contains information about the word itself and how it relates to the other words in the sentence.</p> <p>This process is repeated for every word, enabling the encoder to understand and represent each word in the context of the entire sentence. Through this mechanism, transformers achieve a deep understanding of the text, considering the meaning of individual words and their broader context within the sentence.</p> <p>So clever.</p> <h4 id="sources">Sources</h4> <ul> <li>Self-Attention is all you need</li> <li>Wikipedia</li> <li>Huggingface.co NLP Course</li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/optimising-llm-performance/">Optimising LLM Performance</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/rag-llm-chatbot/">Using Retrieval Augmented Generation (RAG) for chatbots</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/headleass-walkthrough/">Headless - XSS, Command Injection, and Sudo abuse.</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/hospital-walkthrough/">Hospital - Insecure File Uploads, Business Email Compromise, and kernel exploits.</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/bizness-walkthrough/">Bizness - Authentication bypass and SSRF.</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-JZNPNDMG9P"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-JZNPNDMG9P");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>